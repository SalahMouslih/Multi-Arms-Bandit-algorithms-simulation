# Multi-Arms-Bandit-algorithms-simulation

This notebook provides an intuitive guide for the Multi-arms Bandit problem using the [SMPyBandits](https://smpybandits.github.io/docs/) library.

We provide a simulation of the **Leader** and **UCB** algorithms, which are two of the most widely used approaches to this problem. Additionally, we provide a visual comparison of their performance through simulations.
